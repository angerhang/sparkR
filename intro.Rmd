---
title: "Intro to SparkR"
author: "Hang Yuan"
date: "November 29, 2015"
output: html_document
---
This is a report on SparkR using R Markdown. 
## Spark Installation
First go to [Spark](http://spark.apache.org) to download the appropriate Spark
installation. In this tutorial we have 1.5.2 with the package type that is pre-built
for Hadoop 2.6 and later. For the download type, choose direct download. 

After you unpack the package, place it in your desired directory and you can proceed to the 
next steps.

## Load SparkR in RStudio 
We need to set the environment and the libaray paths
```{r}
Sys.setenv(SPARK_HOME= "/Users/yuancalvin/spark-1.5.2")
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
```

Now we can load SparkR library 

```{r}
library(SparkR)
sc <- sparkR.init(master="local")
```

## SparkR Basics
Let's first look at some simple operations using SparkR. Unlike a regular R object,
wif we have a file we cannot direclty see what a file looks like by using `view`

We first read the textFile into myFile and use `take` to look at the first 10 lines of
`city.txt`
```{r}
start.time <- Sys.time()
myFile <- SparkR:::textFile(sc, "/Users/yuancalvin/Desktop/city.txt")
take(myFile, 10)
words <- SparkR:::flatMap(myFile, function(line) {strsplit(line, " ")[[1]]})
wordCount <- SparkR:::lapply(words, function(word) {list(word, 1)})
counts <- SparkR:::reduceByKey(wordCount, "+" , 2)
output <- collect(counts)
k = lapply(output, unlist)
m <- data.frame(t(data.frame(k)))
head(m)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

Let's see the wordcount example using plain R
```{r}
start.time <- Sys.time()
X <- scan("/Users/yuancalvin/Desktop/city.txt", what = character());
counts <- rle(X);
countsTbale <- data.frame(number=counts$values, n=counts$lengths);
countsTbale <- countsTbale[order(-countsTbale$n), ];
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

An example of linear regression
```{r}
library(SparkR)
Sys.setenv('SPARKR_SUBMIT_ARGS'='"--packages" "com.databricks:spark-csv_2.10:1.0.3" "sparkr-shell"')
sqlContext <- sparkRSQL.init(sc)

# Read csv into sparkR format
df <- read.df(sqlContext, "2008.csv", source = "com.databricks.spark.csv", header= "true")

# Make the training set on the ones that don't have null
training <- dropna(df)
showDF(select(training,"UniqueCarrier","ArrTime","AirTime","Distance", "DepDelay"))

model <- glm(DepDelay ~ UniqueCarrier , family = "gaussian", data = training)
summary(model)
```

